
# Minishell - Parse & Signals

## üì¶ ESTRUCTURAS DE DATOS

### `t_mini` - Estructura Principal del Shell

```c
typedef struct s_mini
{
    char        *prompt;      // Prompt con colores que se muestra
    char        *input;       // Lo que el usuario escribe (de readline)
    char        *pwd;         // Directorio actual de trabajo
    int         argc;         // N√∫mero de argumentos del programa
    char        **argv;       // Copia de argumentos del programa
    char        **env;        // Copia del environment (variables)
    int         exit_sts;     // C√≥digo de salida del √∫ltimo comando
    t_cmd       *cmds;        // Lista enlazada de comandos parseados
} t_mini;
```


**¬øPara qu√© sirve cada campo?**

| Campo | Prop√≥sito | Ejemplo |
|-------|-----------|---------|
| `prompt` | Guarda el string del prompt coloreado | `"\001\033[36m\002minishell\001\033[0m\002 $ "` |
| `input` | Almacena lo que readline lee del usuario | `"echo hello \| grep h"` |
| `pwd` | Directorio actual, para el prompt y pwd | `"/home/ravazque/minishell"` |
| `argc/argv` | Info del programa (minishell -c "comando") | `argc=3, argv=["minishell", "-c", "echo"]` |
| `env` | Array de strings del environment | `["USER=ravazque", "HOME=/home/...", NULL]` |
| `exit_sts` | √öltimo c√≥digo de salida (para $?) | `0` (√©xito), `127` (cmd not found), `130` (Ctrl+C) |
| `cmds` | Cabeza de la lista de comandos parseados | `cmd1 ‚Üí cmd2 ‚Üí cmd3 ‚Üí NULL` |

**Ejemplo de estado durante ejecuci√≥n:**

```c
// Usuario escribe: echo hello | grep h
mini = {
    prompt = "minishell $ ",
    input = "echo hello | grep h",
    pwd = "/home/ravazque",
    env = ["USER=ravazque", "PATH=/usr/bin", ...],
    exit_sts = 0,
    cmds ‚Üí cmd1 ‚Üí cmd2 ‚Üí NULL
}
```

---

### `t_cmd` - Representa un Comando

```c
typedef struct s_cmd
{
    t_redir     *redirs;      // Lista de redirecciones del comando
    t_token     *tokn;        // Lista de tokens (temporal, se libera)
    char        **tokens;     // Array final de argumentos
    struct s_cmd *next;       // Siguiente comando en el pipeline
} t_cmd;
```


**¬øPara qu√© sirve cada campo?**

| Campo | Prop√≥sito | Cu√°ndo se usa | Cu√°ndo se libera |
|-------|-----------|---------------|------------------|
| `redirs` | Guarda redirecciones (< > >> <<) del comando | Lexer lo crea | Al final |
| `tokn` | Lista temporal de tokens con info de comillas | Tokenizer ‚Üí Expander | Despu√©s de expandir |
| `tokens` | Array final listo para execve | Despu√©s del Expander | Al final |
| `next` | Puntero al siguiente comando (pipe) | Si hay pipes | Limpieza recursiva |

**¬øPor qu√© dos representaciones de tokens (`tokn` y `tokens`)?**

```c
// tokn (temporal) - Durante parsing:
tokn ‚Üí {raw="hello", parts=[{content="hello", dq=1}]} ‚Üí NULL
       ‚Üì
       Necesario para saber si ven√≠a de comillas dobles

// tokens (final) - Despu√©s de expandir:
tokens = ["echo", "hello ravazque", NULL]
         ‚Üì
         Listo para execve(path, tokens, env)
```

**Ejemplo de comando en memoria:**

```c
// Comando: cat < in.txt > out.txt
cmd = {
    redirs ‚Üí {target="in.txt", i_redir=1} ‚Üí {target="out.txt", o_redir=1} ‚Üí NULL,
    tokn = NULL,  // Ya se liber√≥
    tokens = ["cat", NULL],
    next = NULL  // No hay pipe
}
```

---

### `t_token` - Token con Informaci√≥n de Comillas

```c
typedef struct s_token
{
    char            *raw;          // Token completo concatenado
    int             is_squote;     // ¬øTodo el token es de comillas simples?
    int             is_dquote;     // ¬øTodo el token es de comillas dobles?
    t_token_part    *parts;        // Lista de partes del token
    struct s_token  *next;         // Siguiente token
} t_token;
```


**¬øPara qu√© sirve cada campo?**

| Campo | Prop√≥sito | Ejemplo |
|-------|-----------|---------|
| `raw` | Contenido completo del token (todas las partes unidas) | `"helloworldtest"` |
| `is_squote` | Marca si TODO el token era `'...'` | `1` si es `'hello'`, `0` si es `"hello"` |
| `is_dquote` | Marca si TODO el token era `"..."` | `1` si es `"hello"`, `0` si es `hello` |
| `parts` | Lista de fragmentos que componen el token | Ver t_token_part |
| `next` | Siguiente token en la lista | Siguiente palabra del comando |

**¬øPor qu√© necesitamos `parts`?**

Porque un token puede ser una mezcla:

```bash
echo "hello"'world'test
```

**Ejemplo:**

```c
// Token: "hello"'world'test
token = {
    raw = "helloworldtest",  // Todo junto
    is_squote = 0,  // No es SOLO comilla simple
    is_dquote = 0,  // No es SOLO comilla doble
    parts ‚Üí part1 ‚Üí part2 ‚Üí part3 ‚Üí NULL
}
```

---

### `t_token_part` - Parte de un Token

```c
typedef struct s_token_part
{
    char                *content;      // Contenido de esta parte
    int                 is_squote;     // ¬øEsta parte es de comillas simples?
    int                 is_dquote;     // ¬øEsta parte es de comillas dobles?
    struct s_token_part *next;         // Siguiente parte
} t_token_part;
```


**¬øPara qu√© sirve?**

Guarda **cada fragmento** del token con informaci√≥n de si ven√≠a de comillas.

**Ejemplo visual:**
```bash
Token: "hello"'world'test

part1 = {content="hello", is_squote=0, is_dquote=1}  // De comillas dobles
part2 = {content="world", is_squote=1, is_dquote=0}  // De comillas simples
part3 = {content="test",  is_squote=0, is_dquote=0}  // Sin comillas
```

**¬øPor qu√© es crucial esta informaci√≥n?**

```bash
echo "$USER"'$HOME'$PWD
      ‚Üì       ‚Üì      ‚Üì
   EXPANDE  NO EXP  EXPANDE

Resultado: "ravazque$HOME/home/ravazque"
```

El expander recorre las partes y decide seg√∫n `is_squote`:
```c
if (part->is_squote)
    copiar literal (NO expandir)
else
    buscar y expandir variables
```

---

### `t_redir` - Redirecci√≥n

```c
typedef struct s_redir
{
    char            *target;      // Archivo o delimitador heredoc
    int             hd_expand;    // Heredoc: ¬øexpandir variables?
    int             i_redir;      // Tipo de redirecci√≥n entrada
    int             o_redir;      // Tipo de redirecci√≥n salida
    struct s_redir  *next;        // Siguiente redirecci√≥n
} t_redir;
```


**¬øPara qu√© sirve cada campo?**

| Campo | Valores | Significado |
|-------|---------|-------------|
| `target` | `"file.txt"`, `"EOF"` | Nombre del archivo o delimitador |
| `hd_expand` | `0` o `1` | En heredoc: `1`=expandir vars, `0`=no expandir |
| `i_redir` | `0`, `1`, `2` | `0`=nada, `1`=`<`, `2`=`<<` |
| `o_redir` | `0`, `1`, `2` | `0`=nada, `1`=`>`, `2`=`>>` |
| `next` | puntero | Siguiente redirecci√≥n del comando |

**Tabla de tipos:**

| Operador | `i_redir` | `o_redir` | `hd_expand` | Acci√≥n |
|----------|-----------|-----------|-------------|--------|
| `<` | 1 | 0 | 0 | Leer desde archivo |
| `<<` | 2 | 0 | 1 (por defecto) | Heredoc |
| `>` | 0 | 1 | 0 | Escribir truncando |
| `>>` | 0 | 2 | 0 | Escribir a√±adiendo |

**Ejemplos:**

```c
// cat < in.txt
redir = {
    target = "in.txt",
    hd_expand = 0,
    i_redir = 1,  // Entrada desde archivo
    o_redir = 0,
    next = NULL
}

// cat << EOF
redir = {
    target = "EOF",
    hd_expand = 1,  // Expandir variables dentro del heredoc
    i_redir = 2,    // Heredoc
    o_redir = 0,
    next = NULL
}

// cat << 'EOF'  (comillas en delimitador)
redir = {
    target = "EOF",
    hd_expand = 0,  // NO expandir por las comillas
    i_redir = 2,
    o_redir = 0,
    next = NULL
}
```

**M√∫ltiples redirecciones:**

```bash
cat < in1.txt < in2.txt > out1.txt > out2.txt

redirs ‚Üí {target="in1.txt", i_redir=1}
      ‚Üí {target="in2.txt", i_redir=1}  ‚Üê Prevalece esta
      ‚Üí {target="out1.txt", o_redir=1}
      ‚Üí {target="out2.txt", o_redir=1}  ‚Üê Prevalece esta
      ‚Üí NULL
```

---

## üî§ TOKENIZER

### ¬øQu√© Hace?

Convierte el string de entrada en una **lista de tokens**, donde cada token guarda:
1. Su contenido (`raw`)
2. Si ven√≠a de comillas simples/dobles
3. Las partes que lo componen

### L√≥gica Completa

```
ENTRADA: char *input = "echo \"hello $USER\" 'world'"

PASO 1: Recorrer car√°cter por car√°cter
  - Acumular en buffer temporal
  - Detectar espacios ‚Üí fin de token
  - Detectar comillas ‚Üí lectura especial

PASO 2: Por cada token detectado
  - Crear estructura t_token
  - Guardar partes (t_token_part)
  - A√±adir a lista cmd->tokn

SALIDA: Lista enlazada de t_token
```

### Proceso Detallado

#### Fase 1: Inicializaci√≥n

```c
int tokenizer(t_mini **mini)
{
    char *in = (*mini)->input;           // Input del usuario
    size_t i = 0;                        // √çndice actual
    char *bf = NULL;                     // Buffer temporal para caracteres
    t_token_part *tp = NULL;             // Partes del token actual
    t_cmd *cmd = malloc(...);            // Comando contenedor
    int in_tok = 0;                      // Flag: ¬øestamos en un token?
    
    // Bucle principal...
}
```

#### Fase 2: Bucle Principal

```c
while (in[i])
{
    if (is_space(in[i]))
    {
        // CASO 1: ESPACIO
        // Si est√°bamos en un token, finalizarlo
        if (in_tok)
        {
            // 1. Convertir buffer a part
            flush_part(&bf, 0, 0, &tp)
            
            // 2. Convertir parts a token
            token = mk_tok_from_parts(tp)
            
            // 3. A√±adir token a cmd->tokn
            add_tok_to_cmd(token, cmd)
            
            in_tok = 0
        }
        
        // Saltar todos los espacios
        while (is_space(in[i]))
            i++
    }
    else if (in[i] == '\'' || in[i] == '"')
    {
        // CASO 2: COMILLA
        char q = in[i]           // Guarda tipo (' o ")
        i++                      // Salta comilla apertura
        
        // Si hab√≠a buffer previo, guardarlo como parte sin comillas
        if (bf != NULL)
            flush_part(&bf, 0, 0, &tp)
        
        // Leer TODO hasta comilla de cierre
        while (in[i] && in[i] != q)
        {
            append_char(&bf, in[i])
            i++
        }
        
        i++  // Salta comilla de cierre
        
        // Guardar esta parte MARCADA como de comillas
        int sq = (q == '\'') ? 1 : 0
        int dq = (q == '"') ? 1 : 0
        flush_part(&bf, sq, dq, &tp)
        
        in_tok = 1
    }
    else
    {
        // CASO 3: CAR√ÅCTER NORMAL
        append_char(&bf, in[i])
        i++
        in_tok = 1
    }
}

// Guardar √∫ltimo token si qued√≥ pendiente
if (in_tok)
    finalize_tok(&bf, &tp, cmd)
```

### Funciones Auxiliares del Tokenizer

#### `flush_part()` - Crea y guarda una parte

```c
static int flush_part(char **bf, int sq, int dq, t_token_part **parts)
{
    // Crea estructura t_token_part
    part = malloc(sizeof(t_token_part))
    part->content = strdup(bf ? bf : "")
    part->is_squote = sq
    part->is_dquote = dq
    part->next = NULL
    
    // A√±ade a lista de partes
    add_tok_part(parts, part)
    
    // Limpia buffer
    free(bf)
    bf = NULL
}
```

#### `mk_tok_from_parts()` - Crea token desde partes

```c
static t_token *mk_tok_from_parts(t_token_part *parts)
{
    // Une todas las partes en un string
    char *joined = ""
    for (part in parts)
        joined = strcat(joined, part->content)
    
    // Crea estructura t_token
    tok = malloc(sizeof(t_token))
    tok->raw = joined           // "helloworldtest"
    tok->is_squote = 0          // Calculado seg√∫n partes
    tok->is_dquote = 0          // Calculado seg√∫n partes
    tok->parts = parts          // Guarda lista de partes
    tok->next = NULL
    
    return tok
}
```

### Ejemplo Visual Completo

```bash
INPUT: echo "hello $USER" 'world'
```

**Procesamiento car√°cter por car√°cter:**

```
i=0: 'e' ‚Üí bf="e", in_tok=1
i=1: 'c' ‚Üí bf="ec"
i=2: 'h' ‚Üí bf="ech"
i=3: 'o' ‚Üí bf="echo"
i=4: ' ' ‚Üí ESPACIO
     ‚Üì
     flush_part(bf="echo", sq=0, dq=0)
     ‚Üì
     part1 = {content="echo", is_squote=0, is_dquote=0}
     ‚Üì
     mk_tok_from_parts([part1])
     ‚Üì
     token1 = {raw="echo", is_squote=0, is_dquote=0, parts=[part1]}
     ‚Üì
     add_tok_to_cmd(token1, cmd)
     ‚Üì
     cmd->tokn = token1 ‚Üí NULL
     
     bf=NULL, in_tok=0

i=5: '"' ‚Üí COMILLA DOBLE
     ‚Üì
     q='"', i=6
     ‚Üì
     Leer hasta siguiente '"'
     
i=6-16: "hello $USER"
     ‚Üì
     bf="hello $USER"
     
i=17: '"' ‚Üí Cierra comilla
     ‚Üì
     flush_part(bf="hello $USER", sq=0, dq=1)
     ‚Üì
     part2 = {content="hello $USER", is_squote=0, is_dquote=1}
     ‚Üì
     mk_tok_from_parts([part2])
     ‚Üì
     token2 = {raw="hello $USER", is_squote=0, is_dquote=1, parts=[part2]}
     ‚Üì
     cmd->tokn = token1 ‚Üí token2 ‚Üí NULL

i=18: ' ' ‚Üí ESPACIO (resetea)

i=19: '\'' ‚Üí COMILLA SIMPLE
     ‚Üì
     Mismo proceso...
     ‚Üì
     token3 = {raw="world", is_squote=1, is_dquote=0, parts=[part3]}
     ‚Üì
     cmd->tokn = token1 ‚Üí token2 ‚Üí token3 ‚Üí NULL
```

**Resultado final:**

```c
cmd->tokn:
  ‚îú‚îÄ token1: {raw="echo", parts=[{content="echo", sq=0, dq=0}]}
  ‚îú‚îÄ token2: {raw="hello $USER", parts=[{content="hello $USER", sq=0, dq=1}]}
  ‚îî‚îÄ token3: {raw="world", parts=[{content="world", sq=1, dq=0}]}
```

**¬øQu√© guarda `cmd` despu√©s del tokenizer?**

```c
cmd = {
    redirs = NULL,          // A√∫n no procesadas
    tokn = lista_tokens,    // Lista enlazada de t_token
    tokens = NULL,          // A√∫n no creado
    next = NULL             // A√∫n no hay pipes
}
```

---

## ‚öôÔ∏è LEXER

### ¬øQu√© Hace?

Toma la lista de tokens y:
1. **Separa comandos** por pipes `|`
2. **Identifica redirecciones** (`<`, `>`, `>>`, `<<`)
3. **Crea estructuras `t_redir`**
4. **Elimina operadores** de la lista de tokens
5. **Convierte tokens** a array final `char **`

### L√≥gica Completa

```
ENTRADA: cmd->tokn = ["echo", "hello", "|", "grep", "h", ">", "out.txt"]

FASE 1 - split_pipes():
  Busca "|" en la lista
  Crea comando separado por cada segmento
  ‚Üì
  cmd1->tokn = ["echo", "hello"]
  cmd2->tokn = ["grep", "h", ">", "out.txt"]

FASE 2 - proc_redirs():
  Para cada comando:
    Busca <, >, >>, <<
    Crea t_redir
    ELIMINA operador y target de tokn
  ‚Üì
  cmd1->tokn = ["echo", "hello"]
  cmd1->redirs = NULL
  
  cmd2->tokn = ["grep", "h"]
  cmd2->redirs = [{target="out.txt", o_redir=1}]

FASE 3 - finalize_cmds():
  Convierte tokn (lista) ‚Üí tokens (array)
  ‚Üì
  cmd1->tokens = ["echo", "hello", NULL]
  cmd2->tokens = ["grep", "h", NULL]

SALIDA: Lista de comandos con tokens y redirs separados
```

### Fase 1: Divisi√≥n por Pipes

```c
static int split_pipes(t_mini *mini)
{
    // Entrada: cmd->tokens = ["echo", "hello", "|", "grep", "h"]
    char **toks = mini->cmds->tokens
    t_cmd *lst = NULL  // Nueva lista de comandos
    int start = 0
    int i = 0
    
    while (toks[i])
    {
        if (strcmp(toks[i], "|") == 0)
        {
            // Encontr√≥ pipe
            // Crear comando con tokens[start...i-1]
            
            new_cmd = malloc(sizeof(t_cmd))
            new_cmd->tokn = NULL
            
            // Copiar tokens del rango al nuevo comando
            for (j = start; j < i; j++)
            {
                tok_node = mk_tok_node(toks[j])
                add_tok(new_cmd, tok_node)
            }
            
            // A√±adir a lista
            add_cmd_to_lst(&lst, new_cmd)
            
            start = i + 1  // Siguiente comando empieza despu√©s del pipe
        }
        i++
    }
    
    // √öltimo comando (o √∫nico si no hay pipes)
    mk_cmd_range(toks, start, i, &lst)
    
    // Liberar tokens originales y reemplazar
    free_dblptr(mini->cmds->tokens)
    free_cmds(mini->cmds)
    mini->cmds = lst  // Ahora mini->cmds apunta a lista de comandos
}
```

**Ejemplo visual:**
```
ANTES:
mini->cmds: {tokn=NULL, tokens=["echo", "|", "grep"]}

DESPU√âS:
mini->cmds ‚Üí cmd1 ‚Üí cmd2 ‚Üí NULL
             ‚Üì       ‚Üì
       tokn=[echo] tokn=[grep]
```

### Fase 2: Procesamiento de Redirecciones

```c
static int proc_redirs(t_cmd *cmd)
{
    t_token *curr = cmd->tokn
    t_token *prev = NULL
    
    while (curr)
    {
        next = curr->next
        
        // ¬øEs operador de redirecci√≥n?
        if (is_redir(curr->raw))  // <, >, >>, <<
        {
            // Debe haber siguiente token (el archivo)
            if (!next)
                return error("syntax error near newline")
            
            // CREAR ESTRUCTURA t_redir
            redir = malloc(sizeof(t_redir))
            redir->target = strdup(next->raw)  // Nombre del archivo
            redir->next = NULL
            
            // Determinar tipo seg√∫n operador
            if (curr->raw == "<")
                redir->i_redir = 1
            else if (curr->raw == "<<")
                redir->i_redir = 2, redir->hd_expand = 1
            else if (curr->raw == ">")
                redir->o_redir = 1
            else if (curr->raw == ">>")
                redir->o_redir = 2
            
            // A√±adir a lista de redirecciones
            add_redir(cmd, redir)
            
            // ELIMINAR curr y next de cmd->tokn
            rm_redir_toks(&cmd->tokn, curr, prev)
            
            // Ajustar punteros para continuar
            curr = prev ? prev->next : cmd->tokn
            continue
        }
        
        prev = curr
        curr = next
    }
}
```

**Ejemplo paso a paso:**
```
ENTRADA: cmd->tokn = [cat] ‚Üí [<] ‚Üí [in.txt] ‚Üí [>] ‚Üí [out.txt] ‚Üí NULL

Iteraci√≥n 1:
  curr = [cat]
  No es redir ‚Üí prev=[cat], curr=[<]

Iteraci√≥n 2:
  curr = [<], next = [in.txt]
  ES REDIR
  ‚Üì
  redir1 = {target="in.txt", i_redir=1, o_redir=0}
  add_redir(cmd, redir1)
  ‚Üì
  rm_redir_toks(): Elimina [<] y [in.txt]
  ‚Üì
  cmd->tokn = [cat] ‚Üí [>] ‚Üí [out.txt] ‚Üí NULL
  cmd->redirs = redir1 ‚Üí NULL
  ‚Üì
  curr = [cat]->next = [>]

Iteraci√≥n 3:
  curr = [>], next = [out.txt]
  ES REDIR
  ‚Üì
  redir2 = {target="out.txt", i_redir=0, o_redir=1}
  add_redir(cmd, redir2)
  ‚Üì
  rm_redir_toks(): Elimina [>] y [out.txt]
  ‚Üì
  cmd->tokn = [cat] ‚Üí NULL
  cmd->redirs = redir1 ‚Üí redir2 ‚Üí NULL

RESULTADO:
  cmd->tokn = [cat]
  cmd->redirs = [{target="in.txt", i_redir=1}] 
              ‚Üí [{target="out.txt", o_redir=1}]
```

### Fase 3: Conversi√≥n a Array

```c
static int finalize_cmds(t_mini *mini)
{
    t_cmd *curr = mini->cmds
    
    while (curr)
    {
        // Convertir lista de tokens ‚Üí array
        curr->tokens = toks_to_arr(curr->tokn)
        
        curr = curr->next
    }
}
```

```c
static char **toks_to_arr(t_token *toks)
{
    // Cuenta tokens
    cnt = 0
    for (t in toks)
        cnt++
    
    // Reserva array
    arr = malloc(sizeof(char*) * (cnt + 1))
    
    // Copia cada token->raw al array
    i = 0
    for (t in toks)
    {
        arr[i] = strdup(t->raw)
        i++
    }
    arr[cnt] = NULL
    
    return arr
}
```

**Transformaci√≥n:**
```
ANTES:
cmd->tokn = [tok1] ‚Üí [tok2] ‚Üí [tok3] ‚Üí NULL
             ‚Üì        ‚Üì        ‚Üì
           raw="cat" raw="hello" raw="world"

DESPU√âS:
cmd->tokens = ["cat", "hello", "world", NULL]
                ‚Üì       ‚Üì         ‚Üì
	       raw="cat" raw="hello" raw="world"
```

**¬øQu√© contiene `cmd` despu√©s del lexer?**

```c
cmd = {
    redirs = lista_redirecciones,  // t_redir creadas
    tokn = lista_tokens,            // A√∫n existe (se libera despu√©s)
    tokens = array_strings,         // Listo para execve
    next = siguiente_comando        // Si hay pipe
}
```

---

## üîÑ EXPANDER

### ¬øQu√© Hace?

Recorre `cmd->tokens` y **sustituye variables** por sus valores:
1. Lee cada token
2. Verifica si ten√≠a comillas simples (NO expandir)
3. Busca `$VARIABLE` en el string
4. Extrae nombre de variable
5. Busca valor en `mini->env`
6. Sustituye en el resultado

### L√≥gica Completa

```
ENTRADA: cmd->tokn con informaci√≥n de comillas

PARA CADA COMANDO:
  PARA CADA TOKEN:
    SI token tiene comillas simples:
      NO EXPANDIR ‚Üí copiar literal
    
    SI token tiene comillas dobles o sin comillas:
      exp_str_part():
        Buscar '# üìö Documentaci√≥n Simplificada - Minishell

> **Autor:** ravazque  
> **Proyecto:** Minishell - Shell Bash simplificado  

---

## üìã √çndice

1. [Visi√≥n General](#visi√≥n-general)
2. [Tokenizer](#tokenizer)
3. [Lexer](#lexer)
4. [Expander](#expander)
5. [Se√±ales](#se√±ales)
6. [Ejemplo Completo](#ejemplo-completo)

---

## üéØ Visi√≥n General

### ¬øQu√© hace tu programa?

Tu minishell procesa comandos en 5 pasos:

```

Input: echo "Hello $USER" | grep world > file.txt

1. TOKENIZER  ‚Üí Divide en tokens respetando comillas
2. LEXER           ‚Üí Separa por pipes y detecta redirecciones
3. EXPANDER   ‚Üí Sustituye variables ($USER ‚Üí ravazque)
4. SE√ëALES      ‚Üí Maneja Ctrl+C, Ctrl+D
5. EJECUCI√ìN (HEREDOC, REDIRECCIONES, ETC...) ‚Üí ‚ùå POR IMPLEMENTAR

```

### Estructura Principal

```c
typedef struct s_mini
{
    char     *input;      // Lo que escribe el usuario
    char     **env;       // Variables de entorno
    int      exit_sts;    // C√≥digo de salida √∫ltimo comando
    t_cmd    *cmds;       // Lista de comandos parseados
} t_mini;

typedef struct s_cmd
{
    char     **tokens;    // ["echo", "hello", NULL]
    t_redir  *redirs;     // Lista de redirecciones
    t_cmd    *next;       // Siguiente comando (para pipes)
} t_cmd;
```

---
---
## üî§ TOKENIZER

### ¬øQu√© hace?

Convierte el string de entrada en **tokens** (palabras), respetando comillas.

### L√≥gica

```
Input: echo "hello world" 'test'

1. Lee car√°cter por car√°cter
2. Espacios ‚Üí separan tokens
3. Comillas ‚Üí agrupa todo lo que est√© dentro como un solo token
4. Guarda cada token con informaci√≥n de si ven√≠a de comillas
```

### Funciones Principales

#### `tokenizer()` - Funci√≥n principal

```c
int tokenizer(t_mini **mini)
{
    // Variables
    char *in = input del usuario
    char *buffer = acumula caracteres del token actual
    int in_tok = flag "¬øestoy construyendo un token?"
    
    // Recorre input car√°cter por car√°cter
    while (hay caracteres)
    {
        if (es_espacio)
            ‚Üí Guarda token actual y resetea buffer
        
        else if (es_comilla ' o ")
            ‚Üí Lee todo hasta cerrar comilla
            ‚Üí Marca que viene de comillas
        
        else
            ‚Üí A√±ade car√°cter al buffer
    }
    
    // Guarda √∫ltimo token
}
```

#### `handle_quote()` - Maneja comillas

```c
static int handle_quote(...)
{
    // Guarda tipo de comilla (' o ")
    char q = comilla actual
    
    // Salta comilla de apertura
    i++
    
    // Lee TODO hasta encontrar comilla de cierre
    while (no encuentro comilla de cierre)
        a√±ade car√°cter al buffer
    
    // Marca esta parte como "de comillas"
    // Importante: esto permite saber despu√©s si expandir o no
}
```

### Ejemplo Visual

```
Input: echo "hello $USER" world

Paso 1: 'e' 'c' 'h' 'o'
  ‚Üí buffer = "echo"
  
Paso 2: ' ' (espacio)
  ‚Üí Guarda token: {content="echo", comillas=NO}
  ‚Üí Resetea buffer

Paso 3: '"' (comilla doble)
  ‚Üí Lee hasta siguiente '"'
  ‚Üí buffer = "hello $USER"
  ‚Üí Guarda token: {content="hello $USER", comillas=DOBLES}

Paso 4: ' ' (espacio)
  ‚Üí Resetea buffer

Paso 5: 'w' 'o' 'r' 'l' 'd'
  ‚Üí buffer = "world"
  ‚Üí Guarda token: {content="world", comillas=NO}

RESULTADO:
Token 1: "echo" (sin comillas)
Token 2: "hello $USER" (con comillas dobles)
Token 3: "world" (sin comillas)
```

### ¬øPor qu√© es importante guardar si ven√≠a de comillas?

Porque despu√©s el **EXPANDER** necesita saber:
- Comillas simples `'$USER'` ‚Üí NO expandir (dejar literal)
- Comillas dobles `"$USER"` ‚Üí S√ç expandir
- Sin comillas `$USER` ‚Üí S√ç expandir

---

## ‚öôÔ∏è LEXER

### ¬øQu√© hace?

1. **Separa comandos** por pipes `|`
2. **Identifica redirecciones** (`<`, `>`, `>>`, `<<`)
3. **Organiza la informaci√≥n** para que sea f√°cil ejecutar

### L√≥gica

```
Input tokenizado: ["echo", "hello", "|", "grep", "h", ">", "out.txt"]

FASE 1 - Divisi√≥n por pipes:
  Busca "|" en los tokens
  Crea un comando separado por cada pipe
  
  Resultado:
    Comando 1: ["echo", "hello"]
    Comando 2: ["grep", "h", ">", "out.txt"]

FASE 2 - Procesar redirecciones:
  Busca operadores de redirecci√≥n (<, >, >>, <<)
  Extrae archivo/target
  Elimina operador y archivo de la lista de tokens
  
  Resultado:
    Comando 1: tokens=["echo", "hello"], redirs=NULL
    Comando 2: tokens=["grep", "h"], redirs=[{target="out.txt", tipo=salida}]
```

### Funciones Principales

#### `lexer()` - Funci√≥n principal

```c
int lexer(t_mini *mini)
{
    // 1. Divide por pipes
    split_pipes(mini)  // echo hello | grep h  ‚Üí  2 comandos
    
    // 2. Para cada comando, procesa redirecciones
    for cada comando
        proc_redirs(comando)  // grep h > out.txt  ‚Üí  extrae redirecci√≥n
    
    // 3. Convierte todo a arrays finales
    finalize_cmds(mini)
}
```

#### `split_pipes()` - Separa por pipes

```c
static int split_pipes(t_mini *mini)
{
    // Recorre todos los tokens
    for cada token
    {
        if (token == "|")
        {
            // Crea nuevo comando con tokens anteriores
            crear_comando(desde_inicio, hasta_aqu√≠)
            
            // Marca nuevo inicio
            inicio = posici√≥n_actual + 1
        }
    }
    
    // Crea √∫ltimo comando
    crear_comando(desde_inicio, hasta_final)
}
```

#### `proc_redirs()` - Procesa redirecciones

```c
static int proc_redirs(t_cmd *cmd)
{
    // Recorre tokens del comando
    for cada token
    {
        if (token es <, >, >>, <<)
        {
            siguiente_token = archivo/delimitador
            
            // Crea estructura de redirecci√≥n
            redir = {
                target = siguiente_token
                tipo = entrada/salida/append/heredoc
            }
            
            // A√±ade a lista de redirecciones
            a√±adir_a_lista(cmd->redirs, redir)
            
            // ELIMINA ambos tokens (operador y archivo)
            eliminar(token_actual y siguiente_token)
        }
    }
}
```

### Tipos de Redirecci√≥n

```c
< archivo     ‚Üí  i_redir=1  (entrada desde archivo)
> archivo     ‚Üí  o_redir=1  (salida truncando)
>> archivo    ‚Üí  o_redir=2  (salida a√±adiendo)
<< delimiter  ‚Üí  i_redir=2  (heredoc)
```

### Ejemplo Visual

```
Input: cat < in.txt | grep hello > out.txt

DESPU√âS DEL TOKENIZER:
["cat", "<", "in.txt", "|", "grep", "hello", ">", "out.txt"]

DESPU√âS DE split_pipes():
  Comando 1: ["cat", "<", "in.txt"]
  Comando 2: ["grep", "hello", ">", "out.txt"]

DESPU√âS DE proc_redirs():
  Comando 1:
    tokens = ["cat"]
    redirs = [{target="in.txt", i_redir=1}]
  
  Comando 2:
    tokens = ["grep", "hello"]
    redirs = [{target="out.txt", o_redir=1}]
```

---

## üîÑ EXPANDER

### ¬øQu√© hace?

Sustituye las **variables de entorno** (`$USER`, `$HOME`, `$?`) por sus valores reales.

### Reglas

```
'$USER'      ‚Üí NO expande (comilla simple)
"$USER"      ‚Üí S√ç expande (comilla doble)
$USER        ‚Üí S√ç expande (sin comillas)
$?           ‚Üí Sustituye por c√≥digo de salida
$NOEXISTE    ‚Üí Sustituye por vac√≠o ""
```

### L√≥gica

```
Para cada comando:
    Para cada token:
        Si el token ten√≠a comillas simples:
            NO EXPANDIR (dejar como est√°)
        
        Si el token ten√≠a comillas dobles o sin comillas:
            Buscar todos los $VARIABLE
            Sustituir por su valor
            Si no existe ‚Üí sustituir por ""
```

### Funciones Principales

#### `expander()` - Funci√≥n principal

```c
int expander(t_mini *mini)
{
    for cada comando
    {
        // 1. Expande los tokens
        exp_cmd_toks(comando)
        
        // 2. Expande archivos de redirecciones
        exp_redirs(comando)
    }
}
```

#### `exp_tok_parts()` - Expande un token

```c
static char *exp_tok_parts(t_token *tok, t_mini *mini)
{
    resultado = ""
    
    // Cada token puede tener m√∫ltiples "partes"
    // (por ejemplo: "hello"'world' son 2 partes)
    
    for cada parte del token
    {
        if (parte es de comillas simples)
            NO expandir ‚Üí a√±adir literal
        
        else
            expandir_string(parte) ‚Üí buscar $VAR y sustituir
            a√±adir al resultado
    }
    
    return resultado
}
```

#### `exp_str_part()` - Expande un string

```c
static char *exp_str_part(const char *s, t_mini *mini, int expandir)
{
    resultado = ""
    
    for cada car√°cter en s
    {
        if (car√°cter es '$' Y permitido expandir Y hay siguiente car√°cter)
        {
            // Extrae nombre de variable
            var_name = leer_hasta_no_alfanum√©rico()  // $USER123 ‚Üí "USER123"
            
            // Busca valor en environment
            valor = buscar_en_env(var_name, mini->env)
            
            // A√±ade valor al resultado
            resultado += valor
        }
        else
        {
            // Car√°cter normal
            resultado += car√°cter
        }
    }
    
    return resultado
}
```

#### `get_env_val()` - Busca variable en environment

```c
static char *get_env_val(const char *key, char **env)
{
    // env = ["HOME=/home/user", "USER=ravazque", "PATH=/usr/bin", ...]
    
    for cada l√≠nea en env
    {
        if (l√≠nea empieza con "KEY=")
        {
            return parte despu√©s del '='
        }
    }
    
    return NULL  // No encontrada
}
```

### Variables Especiales

```c
$?  ‚Üí  C√≥digo de salida del √∫ltimo comando
       Ejemplo: echo $? ‚Üí "0" si el √∫ltimo comando fue exitoso
```

### Ejemplo Visual

```
Input: echo "$USER lives in $HOME" '$PWD'

TOKENS DESPU√âS DEL TOKENIZER:
Token 1: "echo" (sin comillas)
Token 2: "$USER lives in $HOME" (comillas dobles)
Token 3: "$PWD" (comillas simples)

PROCESO DE EXPANSI√ìN:

Token 1: "echo"
  ‚Üí Sin variables ‚Üí resultado: "echo"

Token 2: "$USER lives in $HOME"
  ‚Üí Tiene comillas dobles ‚Üí S√ç EXPANDIR
  
  Procesamiento:
  i=0: '$' ‚Üí Encuentra variable
       var_name = "USER"
       buscar_en_env("USER") ‚Üí "ravazque"
       resultado = "ravazque"
  
  i=5-14: " lives in "
       resultado = "ravazque lives in "
  
  i=15: '$' ‚Üí Encuentra variable
       var_name = "HOME"
       buscar_en_env("HOME") ‚Üí "/home/ravazque"
       resultado = "ravazque lives in /home/ravazque"

Token 3: "$PWD"
  ‚Üí Tiene comillas simples ‚Üí NO EXPANDIR
  ‚Üí resultado: "$PWD" (literal)

TOKENS FINALES:
["echo", "ravazque lives in /home/ravazque", "$PWD"]
```

### Tokens Vac√≠os

```c
Input: echo "" $NOEXISTE "hello"

Expansi√≥n:
  ""         ‚Üí Tiene comillas ‚Üí SE CONSERVA como ""
  $NOEXISTE  ‚Üí Sin comillas y no existe ‚Üí SE ELIMINA
  "hello"    ‚Üí Tiene comillas ‚Üí SE CONSERVA como "hello"

Resultado: ["echo", "", "hello"]
```

---

## üö® SE√ëALES

### ¬øQu√© hace?

Maneja las interrupciones del usuario para que el shell no se rompa.

### Se√±ales Principales

```
Ctrl+C (SIGINT)   ‚Üí Interrumpe comando, NO sale del shell
Ctrl+D (EOF)      ‚Üí Sale del shell limpiamente
Ctrl+\ (SIGQUIT)  ‚Üí Ignorada en modo interactivo
```

### L√≥gica

```
SETUP INICIAL:
  Configura handler personalizado para Ctrl+C
  Ignora Ctrl+\

CUANDO USUARIO PRESIONA Ctrl+C:
  1. Handler se ejecuta
  2. Imprime nueva l√≠nea
  3. Limpia l√≠nea de readline
  4. Redibuja prompt
  5. Marca variable global g_signal_received = SIGINT

EN EL LOOP PRINCIPAL:
  Verifica si g_signal_received tiene valor
  Si es SIGINT ‚Üí establece exit_status = 130
  Resetea g_signal_received a 0
```

### Funciones Principales

#### `setup_interactive_signals()` - Configuraci√≥n inicial

```c
void setup_interactive_signals(void)
{
    // Configura handler para SIGINT (Ctrl+C)
    sigaction(SIGINT, handler_personalizado)
    
    // Ignora SIGQUIT (Ctrl+\)
    signal(SIGQUIT, SIG_IGN)
}
```

#### `handle_sigint()` - Handler de Ctrl+C

```c
static void handle_sigint(int sig)
{
    // Marca que recibimos la se√±al
    g_signal_received = SIGINT
    
    // Nueva l√≠nea
    write(STDOUT, "\n")
    
    // Limpia readline
    rl_on_new_line()
    rl_replace_line("", 0)
    rl_redisplay()
}
```

#### `ft_signal()` - Verificaci√≥n en loop

```c
void ft_signal(t_mini *mini)
{
    // Si readline devolvi√≥ NULL (Ctrl+D)
    if (!mini->input)
    {
        write("exit\n")
        cleanup_and_exit()
    }
    
    // Si recibimos Ctrl+C
    if (g_signal_received == SIGINT)
    {
        mini->exit_sts = 130  // C√≥digo est√°ndar
        g_signal_received = 0  // Reset
    }
}
```

### Comportamiento Visual

```bash
# Usuario escribiendo comando
$ sleep 10
  ^C        ‚Üê Presiona Ctrl+C

$ [prompt limpio, ready para nuevo comando]
```

```bash
# EOF (Ctrl+D)
$ [Ctrl+D]
exit
[Shell termina]
```

### C√≥digos de Salida

```
0    ‚Üí √âxito
1    ‚Üí Error general
2    ‚Üí Mal uso de comando
127  ‚Üí Comando no encontrado
130  ‚Üí Terminado con Ctrl+C
```

---

## üìù EJEMPLO COMPLETO

### Input del usuario:

```bash
echo "Hello $USER" | grep $HOME > out.txt
```

### Paso 1: TOKENIZER

```
Resultado:
Token 1: "echo"
Token 2: "Hello $USER" (comillas dobles)
Token 3: "|"
Token 4: "grep"
Token 5: "$HOME" (sin comillas)
Token 6: ">"
Token 7: "out.txt"
```

### Paso 2: LEXER

```
Divisi√≥n por pipes:
  Comando 1: ["echo", "Hello $USER"]
  Comando 2: ["grep", "$HOME", ">", "out.txt"]

Procesamiento de redirecciones:
  Comando 1: 
    tokens = ["echo", "Hello $USER"]
    redirs = NULL
  
  Comando 2:
    tokens = ["grep", "$HOME"]
    redirs = [{target="out.txt", o_redir=1}]
```

### Paso 3: EXPANDER

```
Comando 1:
  "echo" ‚Üí "echo" (sin cambios)
  "Hello $USER" (comillas dobles) ‚Üí "Hello ravazque"
  
  RESULTADO: tokens = ["echo", "Hello ravazque"]

Comando 2:
  "grep" ‚Üí "grep" (sin cambios)
  "$HOME" (sin comillas) ‚Üí "/home/ravazque"
  
  RESULTADO: 
    tokens = ["grep", "/home/ravazque"]
    redirs = [{target="out.txt", o_redir=1}]
```

### Estructura Final en Memoria

```c
mini->cmds:
  ‚îú‚îÄ Comando 1:
  ‚îÇ    tokens = ["echo", "Hello ravazque", NULL]
  ‚îÇ    redirs = NULL
  ‚îÇ    next ‚Üí Comando 2
  ‚îÇ
  ‚îî‚îÄ Comando 2:
       tokens = ["grep", "/home/ravazque", NULL]
       redirs ‚Üí {target="out.txt", o_redir=1}
       next = NULL
```

---
---
# RESUMEN DE FUNCIONALIDADES

### ‚úÖ Implementado

| Componente | Funcionalidad |
|------------|---------------|
| **Tokenizer** | Divide input en tokens respetando comillas |
| **Lexer** | Separa comandos por pipes y detecta redirecciones |
| **Expander** | Expande variables de entorno ($VAR, $?) |
| **Se√±ales** | Maneja Ctrl+C, Ctrl+D, Ctrl+\ correctamente |
| **Built-ins** | echo, cd, pwd, env, exit funcionan |
| **Prompt** | Prompt con colores y detecci√≥n de git |
| **Memoria** | Sistema completo de limpieza (free) |

### ‚ùå Pendiente (Prioridad Alta)

| Componente        | Qu√© falta                                    |
| ----------------- | -------------------------------------------- |
| **Ejecuci√≥n**     | Ejecutar comandos externos con fork/execve   |
| **Pipes**         | Conectar comandos con tuber√≠as funcionales   |
| **Redirecciones** | Aplicar < > >> realmente                     |
| **Heredocs**      | Implementar << con lectura hasta delimitador |
| **export/unset**  | Modificar variables de entorno               |

---

## üí° CONCEPTOS CLAVE

### ¬øPor qu√© 3 pasos (Tokenizer ‚Üí Lexer ‚Üí Expander)?

Cada paso tiene una responsabilidad clara:

1. **Tokenizer**: "¬øCu√°les son las palabras?"
2. **Lexer**: "¬øC√≥mo se organizan? (pipes, redirs)"
3. **Expander**: "¬øQu√© valores tienen las variables?"

Separar estas tareas hace el c√≥digo m√°s limpio y f√°cil de debuggear.

### ¬øPor qu√© guardar info de comillas?

```bash
echo '$USER'    # Debe imprimir: $USER
echo "$USER"    # Debe imprimir: ravazque
```

El tokenizer marca de d√≥nde viene cada token, para que el expander sepa si debe expandir o no.

### ¬øPor qu√© usar listas enlazadas?

```c
cmd1 ‚Üí cmd2 ‚Üí cmd3 ‚Üí NULL
```

Porque no sabes de antemano cu√°ntos comandos habr√° en un pipe:
- `echo hello` ‚Üí 1 comando
- `echo hello | grep h | wc -l` ‚Üí 3 comandos
